{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c079f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import pandas as pd\n",
    "red = pd.read_csv('winequality-red.csv', delimiter = ';')\n",
    "white = pd.read_csv('winequality-white.csv', delimiter = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116f49b",
   "metadata": {},
   "source": [
    "## Background / Motivation\n",
    "\n",
    "What motivated you to work on this problem?\n",
    "\n",
    "Mention any background about the problem, if it is required to understand your analysis later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2bc7fb",
   "metadata": {},
   "source": [
    "For our project, we built a model to predict the quality of wine based on a variety of physicochemical variables. We were motivated to work on this problem since the quality of wine is something that is pretty subjective and qualitative, and we wanted to develop a model to make the process of assigning quality to wine more objective and repeatable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff1421",
   "metadata": {},
   "source": [
    "## Problem statement \n",
    "\n",
    "Describe your problem statement. Articulate your objectives using absolutely no jargon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ee14e",
   "metadata": {},
   "source": [
    "As mentioned above, our project focused primarily on developing a regression model to predict wine quality (for this particular dataset, this value is discrete and on a scale of 0-10). We used 11 predictors – which contained information on the physicochemical properties of various wine samples – to make this prediction. We wanted to minimize MAE, although we also took note of RMSE (our approach is discussed further in a later section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7b95f",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "What data did you use? Provide details about your data. Include links to data if you are using open-access data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74182555",
   "metadata": {},
   "source": [
    "We used the “Wine Quality Data Set” datasets from UC Irvine’s Machine Learning Repository. The data came in 2 datasets – one for red wine and one for white wine. Each dataset contained  11 continuous predictors (fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulfates, alcohol) in addition to the response variable quality (which took on discrete integer values between 0 and 10). The link to download the data is http://www3.dsi.uminho.pt/pcortez/wine/  and the link to the source is https://archive.ics.uci.edu/ml/datasets/Wine+Quality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c255035",
   "metadata": {},
   "source": [
    "## Stakeholders\n",
    "Who cares? If you are successful, what difference will it make to them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cac9e1",
   "metadata": {},
   "source": [
    "We identified 3 key stakeholders. First, we believe that wine tasters will benefit from our model, as they can use predictions to test their ability to identify good quality wine and use the reported quality of wine as a good benchmark of comparison for testing their opinions against other industry opinions, since it is the median of three wine expert evaluations. Secondly, we believe that wine producers are a key stakeholder to consider. Using our model, wine producers can understand which of their wines is of the highest quality and price their products accordingly. Additionally, they can use this information to gauge customer and wine expert responses based on the predicted quality score for new or existing wine product offerings. Lastly, we believe that wine consumers will benefit from our model, since they can use our model to predict the quality of wine they are looking to purchase before buying it and judge the price of the wine relative to other wines of different or similar qualities to determine whether they are overpaying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ea9bb",
   "metadata": {},
   "source": [
    "## Data quality check / cleaning / preparation \n",
    "\n",
    "Show the distribution of the response here. Report the standard deviation and mean in case of a regression problem, and proportion of 0s and 1s in case of classification.\n",
    "\n",
    "For all other content, as mentioned below, just provide the highlights *(if any)* and put the details in the appendix.\n",
    "\n",
    "In a tabular form, show the distribution of values of each variable used in the analysis - for both categorical and continuous variables. Distribution of a categorical variable must include the number of missing values, the number of unique values, the frequency of all its levels. If a categorical variable has too many levels, you may just include the counts of the top 3-5 levels. \n",
    "\n",
    "Mention any useful insights you obtained from the data quality check that helped you develop the model or helped you realize the necessary data cleaning / preparation. Its ok if there were none.\n",
    "\n",
    "Were there any potentially incorrect values of variables that required cleaning? If yes, how did you clean them? Were there missing values? How did you handle them? Its ok if the data was already clean.\n",
    "\n",
    "Did you do any data wrangling or data preparation before the data was ready to use for model development? Did you create any new predictors from exisiting predictors? For example, if you have number of transactions and spend in a credit card dataset, you may create spend per transaction for predicting if a customer pays their credit card bill. Mention the steps at a broad level, you may put minor details in the appendix. Only mention the steps that ended up being useful towards developing your model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5e06a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard deviation:  quality    0.873684\n",
      "dtype: float64\n",
      "mean of:  quality    5.817734\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "value_counts = red['quality'].value_counts()\n",
    "value_counts_white = white['quality'].value_counts()\n",
    "quality = (\n",
    "3, 4, 5, 6, 7, 8, 9\n",
    ")\n",
    "weight_counts = {\n",
    "    \"White\": np.array([value_counts_white.iloc[5], value_counts_white.iloc[4], value_counts_white.iloc[1],\n",
    "                      value_counts_white.iloc[0], value_counts_white.iloc[2], value_counts_white.iloc[3], value_counts_white.iloc[6]]),\n",
    "        \"Red\": np.array([value_counts.iloc[5], value_counts.iloc[3], value_counts.iloc[0],\n",
    "                      value_counts.iloc[1], value_counts.iloc[2], value_counts.iloc[4], 0])\n",
    "}\n",
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(len(quality))\n",
    "\n",
    "colors = {\n",
    "    \"White\": (220/255, 200/255, 200/255),  # RGB values for cream white\n",
    "    \"Red\": (0.54, 0.0, 0.0)       # RGB values for dark red\n",
    "}\n",
    "\n",
    "for boolean, weight_count in weight_counts.items():\n",
    "    p = ax.bar(quality, weight_count, width, label=boolean, bottom=bottom, color=colors[boolean], edgecolor='black')\n",
    "    bottom += weight_count\n",
    "\n",
    "ax.set_title(\"Count of Median Wine Quality Ratings - Overall\")\n",
    "ax.legend(loc = \"upper right\")\n",
    "plt.savefig(\"response_distribution.png\")\n",
    "\n",
    "print('standard deviation: ', combined_train_y.std())\n",
    "print('mean of: ', combined_train_y.mean())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fa11d",
   "metadata": {},
   "source": [
    "As a result of the quality check, we decided to stratify our train-test splits with respect to the response as the values of the response were highly concentrated in the 5-7 range. With regard to cleaning, our data was already very clean with no missing values, so no cleaning was required. To prepare the data for developing our models, we combined the individual red and white wine data sets as one combined dataset to train our models on, and added a categorical variable that indicated whether the wine was red or white. We then converted this variable into two dummy variables: one for red wine and one for white wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11c9b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd74a9",
   "metadata": {},
   "source": [
    "If there is any EDA that helped with model development, put it here. If EDA didn't help then mention that, and you may show your EDA effort *(if any)* in the appendix.\n",
    "\n",
    "List the insights (as bullet points), if any, you got from EDA  that ended up being useful towards developing your final model. \n",
    "\n",
    "If there are too many plots / tables, you may put them into appendix, and just mention the insights you got from them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f12d5d",
   "metadata": {},
   "source": [
    "Note that you can write code to publish the results of the code, but hide the code using the yaml setting `#|echo: false`. For example, the code below makes a plot, but the code itself is not published with Quarto in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a59729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c782c",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "What kind of a models did you use? What performance metric(s) did you optimize and why?\n",
    "\n",
    "Is there anything unorthodox / new in your approach? \n",
    "\n",
    "What problems did you anticipate? What problems did you encounter? \n",
    "\n",
    "Did your problem already have solution(s) (posted on Kaggle or elsewhere). If yes, then how did you build upon those solutions, what did you do differently? Is your model better as compared to those solutions in terms of prediction accuracy or your chosen metric?\n",
    "\n",
    "**Important: Mention any code repositories (with citations) or other sources that you used, and specifically what changes you made to them for your project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5fa53",
   "metadata": {},
   "source": [
    "For our approach, we each developed a model. The models we individually developed were MARS, Bagged Decision Trees, Random Forest, and XGBoost. We wanted to optimize both RMSE and MAE. We optimized RMSE because other people in our class section tried predicting wine quality using the same dataset, and we were advised to include RMSE as a way to compare our model accuracy against theirs. We also wanted to optimize MAE because we didn’t care about penalizing large errors in prediction, as that didn’t bear much contextual significance. There did exist solutions for this problem on Kaggle, however, many of these solutions focused on classification approaches rather than regression[1] problems and often used other models like LightGBM and CatBoost. In terms of ways that our approach might be considered unorthodox or new / how we built upon existing solutions on Kaggle, we firstly rounded all of our model predictions to be integers, as the response variable (wine quality) only takes on discrete, integer values between 0 and 10, and we wanted our model outputs to be realistic. We additionally combined the red and white wine datasets together and created a dummy variable in the combined dataset to indicate whether the wine was red or white. This allowed us to go beyond the predictors provided and use the type of wine (red or white) as a predictor. Our model RMSE/MAE is better than all the other groups who used the same dataset in our class section – and is also on par with the accuracy of models found online. However, we did notice that there were significantly less observations that had extremely low or extremely high ratings, and we wanted our model to account for that. In order to address this, we stratified. Beyond that, we didn’t encounter many problems, as our approach was pretty straightforward. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab331a",
   "metadata": {},
   "source": [
    "## Developing the model: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ec4c9",
   "metadata": {},
   "source": [
    "Every person must describe their hyperparameter tuning procedure. Show the grid of hyperparameter values over which the initial search was done *(you may paste your grid search / random search / any other search code)*, and the optimal hyperparameter values obtained. After getting the initial search results, how did you make decisions *(if any)* to further fine-tune your model. Did you do another grid / random search or did you tune hyperparameters sequentially? If you think you didn't need any fine tuning after the initial results, then mention that and explain why.\n",
    "\n",
    "Put each model in a section of its name and mention the name of the team-member tuning the model. Below is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea422beb",
   "metadata": {},
   "source": [
    "### MARS\n",
    "*By Cindy Wu*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393baeb",
   "metadata": {},
   "source": [
    "For the MARS model, I tuned max_degrees and max_terms. Below are the values that I simultaneously tuned over. I started off with a lot of values for my initial search then used the results from the coarse grid to narrow down the values I searched for for my finer grid. Visualizing the MAE for each coarse grid value helped a lot in identifying the optimal range, since the graph(s) showed where the MAE was lowest for each hyperparameter value (vizualizations are in Code report).\n",
    "\n",
    "**Coarse Grid:**\n",
    "max_degrees = range(1,6)\n",
    "max_terms = np.arange(10,100,20)\n",
    "\n",
    "**Finer Grid:**\n",
    "max_degrees = range(1,3)\n",
    "max_terms = np.arange(9,15,1)\n",
    "\n",
    "**Optimal hyperparameter values:**\n",
    "max_degrees = 1\n",
    "max_terms = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fbd0e",
   "metadata": {},
   "source": [
    "### Bagged Decision Trees\n",
    "*By Michael Kim*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca785c",
   "metadata": {},
   "source": [
    "For the bagged decision trees model, I first went about the approach of seeing the number of trees after which minimal improvements to OOB MAE would be seen. The visualization in the code section yielded that any number of trees above approximately 100 would be sufficient, with the value later able to be increased afterwards in final models. The hyperparameters that I chose to focus on for tuning of the BaggingRegressor() with underlying base estimators of DecisionTreeRegressor() models were n_estimators, max_samples, max_features, bootstrap, and bootstrap_features. I did not choose to tune for any hyperparameters of the base estimators because it would be computationally expensive to do so in tuning hyperparameters of each individual tree and since our primary objective in bagging was to improve the performance of the ensemble, it was more efficient to focus on the tuning of these bagging-related hyperparameters. Two processes were done with this grid searches, one for scoring with MAE and one for scoring with RMSE. I initially used a grid of hyperparameter values to initially search over shown below as the coarse grid that was often used in this course as a starting point. For RMSE, the optimal hyperparameter values from this initial grid was bootstrap = False, bootstrap_features = False, max_features = 0.75, max_samples = 1.0, n_estimators = 100. For MAE, the optimal hyperparameter values from this initial grid was bootstrap = False, bootstrap_features = True, max_features = 1.0, max_samples = 1.0, n_estimators = 100. After getting the initial search results, since the values obtained were going to hit the boundaries of these values I initially searched over, I wanted to further fine tune the grid to search over to see if these were actually ideal values for hyperparameters. I continued to do more grid searches over new grids with the range of values centered around the optimal value found in the previous grid search for each hyperparameter, which allowed me to iteratively get closer to the hyperparameter values that obtained ideal cross-validated RMSE scores (although the search ended up being extremely fine, it was not computationally expensive enough to deter me from doing this). This approach was used and visualizations were found to not be necessary, instead just using new grids and grid searches to find optimal hyperparameter values for cross-validated metrics. For MAE, the optimal hyperparameter values were bootstrap = False, bootstrap_features = False, max_features = 0.675, max_samples = 0.995, and n_estimators = 100. For RMSE, the optimal hyperparameter values were bootstrap = False, bootstrap_features = False, max_features = 0.6, max_samples = 0.99, and n_estimators = 100. Using these hyperparameter value (and increasing n_estimators to 400), performances of test RMSE: 0.6563301233138936 and test MAE: 0.3556923076923077 were obtained and were used for ensembling (from optimizing for cross-validated RMSE).\n",
    "\n",
    "**Coarse Grid:**\n",
    "n_estimators: [100],\n",
    "max_samples: [0.5, 0.75, 1.0],\n",
    "max_features: [0.5, 0.75, 1.0],\n",
    "bootstrap: [True, False],\n",
    "bootstrap_features: [True, False]\n",
    "\n",
    "**Finer Grid (scoring for RMSE):**\n",
    "n_estimators: [100],\n",
    "max_samples: [0.985, 0.99, 0.995],\n",
    "max_features: [0.625, 0.65, 0.675],\n",
    "bootstrap: [True, False],\n",
    "bootstrap_features: [True, False]\n",
    "\n",
    "**Finer Grid (scoring for MAE):**\n",
    "n_estimators: [100],\n",
    "max_samples: [0.99, 0.995, 1.0],\n",
    "max_features: [0.65, 0.675, 0.7],\n",
    "bootstrap: [True, False],\n",
    "bootstrap_features: [True, False]\n",
    "\n",
    "**Optimal hyperparameter values (for both scoring for MAE):**\n",
    "n_estimators = 100\n",
    "max_samples = 0.995\n",
    "max_features = 0.675\n",
    "bootstrap = False\n",
    "bootstrap_features = False\n",
    "\n",
    "**Optimal hyperparameter values (for both scoring for RMSE):**\n",
    "n_estimators = 100\n",
    "max_samples = 0.99\n",
    "max_features = 0.6\n",
    "bootstrap = False\n",
    "bootstrap_features = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916849c",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "*By Sabrina Kozarovitsky*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384b1e0",
   "metadata": {},
   "source": [
    "For the RandomForest model, I tuned n_estimators, max_features, max_depth, max_leaf_nodes, and bootstrap. Prior to tuning the model on the entire dataset, I began by tuning RandomForest models for only red wine and only white wine. I found that the most impactful hyperparameter on the white and red wine datasets was n_estimators. The other hyperparameters did not effect the RMSE greatly Thus, I began by using the optimal n_estimators from my red and white models as a starting point in the coarse grid for the combined dataset to save time and finely tuned it in later grids. As for the other hyperparameters, I chose very wide ranges. I ran a 5-fold GridSearchCV in order to find the most optimal hyperparameters based on MAE initially. However, in order to better compare our results to the other group's, I switched my final coarse GridSearchCV to optomize for RMSE. I continued to optomize RMSE throughout the hyperparameter tuning process because tuning for RMSE also gave a lower MAE than when tuning for MAE. After finding the optimal hyperparamters through a coarse grid search, I ran a finer grid seach with values closer to the optimal values given by the coarse search. If the coarse grid gave the default value, I kept the default value. In the end, the only hyperparameter that made a difference in the model performance was n_estimators so I continued to run finer and finer grids to find its most optimal value based on CV scores. My final, optimal hyperparameters were: n_estimators=910, random_state=1, max_features = 2, bootstrap = False, max_leaf_nodes = 1300, max_depth = None. These gave an RMSE of 0.6421119001448987 and an MAE of 0.3483076923076923 after rounding the predicted values to the nearest integer (as our y values were discrete)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50433b4",
   "metadata": {},
   "source": [
    "**Coarse Grid:**\n",
    "'n_estimators': [1000, 1350],\n",
    "          'max_features': list(range(2,8,2)),\n",
    "          'max_depth': [None,15,18],\n",
    "          'max_leaf_nodes':[700,1000,1300],\n",
    "          'bootstrap': [True, False]}\n",
    "          \n",
    "**Fine Grid:**\n",
    "{'n_estimators': [900, 1000, 1050, 1100],\n",
    "          'max_features': [1,2],\n",
    "          'max_depth': [None],\n",
    "          'max_leaf_nodes':[1000,1300],\n",
    "          'bootstrap': [True, False]}\n",
    "          \n",
    "**Finer Grid:**\n",
    "{'n_estimators': [900,925, 950, 1000],\n",
    "          'max_features': [2],\n",
    "          'max_depth': [None],\n",
    "          'max_leaf_nodes':[1300],\n",
    "          'bootstrap': [True, False]}\n",
    "          \n",
    "**Finest Grid:**\n",
    "{'n_estimators': list(range(900,930,2)),\n",
    "          'max_features': [2],\n",
    "          'max_depth': [None],\n",
    "          'max_leaf_nodes':[1300],\n",
    "          'bootstrap': [True, False]}\n",
    "          \n",
    "**Optimal Hyperparameters:**\n",
    "n_estimators=910, random_state=1, max_features = 2, bootstrap = False, max_leaf_nodes = 1300, max_depth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f552ef",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "*By Keaton Olds*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471fe69",
   "metadata": {},
   "source": [
    "For the XGBoost model I tuned: gamma , learning_rate, max_depth, n_estimators, reg_lambda, subsample, and colsample_bytree. I decided to tune all of these hyperparamters to ensure a thorough tuning process that would produce the best XGBoost model. To tune these hyperparamters, I began with a coarse grid, followed that up with a fine grid, followed the grid up with manual tuning, and then adjusted the paramters to reduce overfitting; the values for these are all below. With the coarse grid, I used RandomizedSearchCV with 5-fold cross validation. Through this search, I was able to select optimal values for n_estimators gamma, colsample_bytree, and subsample. I used these values in the fine grid search which I used to further narrow done values for learning_rate, reg_lambda, and max_depth. After selecting values for each of the parameters with the grid searches, I tested the model on the train data and got an RMSE of close to 0. I wanted to see if I could get the train RMSE to 0, so I tried manually tuning some of the hyperparamters on the train data, and I noticed learning_rate seemed to have an effect. To examine this relationship further, I visualized cross validation error versus learning rate, and found 0.05 to be an optimal learning rate for the train data. I implemented this change to the model, and tested it on the test data. This led to an RMSE of 0.657, which I wanted to see if I could get a little bit lower. As my train RMSE was 0.0, I figured my model was almost certaintly overfitting to the train data. To fix this, I lowered n_estimators, max_depth, and the subsample to increase bias and decrease variance, therefore correcting for overfitting. I decreased these metrics until my train RMSE was no longer 0.0, which yielded values of 300, 11, and 0.75 respectively. I decided to decrease n_estimators by that much because my visualization of MSE versus n_estimators after the coarse grid search showed that 1000 trees and 500 trees were very similar. Ultimately, this correction for overfitting yielded a final test RMSE of 0.644\n",
    "\n",
    "**Coarse Grid:** max_depth:[4,6,8], n_estimators:[100,500,1000], learning_rate:[0.01, 0.05, 0.1], subsample:[0.5,0.75,1], colsample_bytree:[0.5,0.75,1], reg_lambda:[0,1,10], gamma:[0,10,100]\n",
    "\n",
    "**Fine Grid:** max_depth:[8,9,10,11,12], n_estimators:[1000], learning_rate:[0.01, 0.03, 0.05], subsample:[1], colsample_bytree:[0.5], reg_lambda:[0,2,4,6,8], gamma:[0]\n",
    "\n",
    "**Grid Search Final Params:** max_depth:[12], n_estimators:[1000], learning_rate:[0.01], subsample:[1], colsample_bytree:[0.5], reg_lambda:[2], gamma:[0]\n",
    "\n",
    "**Manual Tuning on Train Data:** learning_rate = 0.05\n",
    "\n",
    "**Overfitting Corrections:** max_depth = 11, n_estimators = 300, subsample = 0.75\n",
    "\n",
    "**Optimal Hyperparameter Values:** max_depth = 11, n_estimators = 300, learning_rate = 0.05, subsample = 0.75, colsample_bytree = 0.5, reg_lambda = 2, gamma = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2594f1",
   "metadata": {},
   "source": [
    "## Model Ensemble "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68b0a5",
   "metadata": {},
   "source": [
    "Put the results of enembling individual models. Feel free to add subsections in this section to add more innovative ensembling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5924a",
   "metadata": {},
   "source": [
    "### Voting ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a31cb2",
   "metadata": {},
   "source": [
    "The simplest voting ensemble will be the model where all models have equal weights.\n",
    "\n",
    "You may come up with innovative methods of estimating weights of the individual models, such as based on their cross-val error. Sometimes, these methods may work better than stacking ensembles, as stacking ensembles tend to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ae2a3",
   "metadata": {},
   "source": [
    "The voting ensemble with the best cross-validated RMSE and MAE scores was that that ensembled XGBRegressor(), RandomForestRegressor(), and BaggingRegressor() models. The ensemble model test RMSE was 0.6387487769068525 and the ensemble model test MAE was 0.3464615384615385, which was lower than all the individual models. However, the best VotingRegressor() model we found on test RMSE and MAE performance actually just ensembled XGBRegressor() and RandomForestRegressor() - however, because we can only choose the best model ensembling combination based on cross-validated metrics and not on test metrics which we wouldn't normally have access to, we chose the aforementioned VotingRegressor() that ensembled three models. We tested across combinations of the four individual models we tuned hyperparameters for, in addition to a simple LinearRegression() model for diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff4cda",
   "metadata": {},
   "source": [
    "### Stacking ensemble\n",
    "Try out different models as the metamodel. You may split work as follows. The person who worked on certain types of models *(say AdaBoost and MARS)* also uses those models as a metamodel in the stacking ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01ae11",
   "metadata": {},
   "source": [
    "We split up the stacking ensemble metamodels by individuals who trained each model (Michael - LinearRegression(), Cindy - MARS, Sabrina - Random Forest, Keaton - XGBRegressor). Additionally, similar to Voting Ensemble, we added in a simple LinearRegression() model as one of the models to be ensembled for diversity sakes. Because it would be too computationally expensive or did not add enough value in improvement to cross-validated metrics, we did not tune hyperparameters for most of the metamodels, except for random forest (tuning max_samples). The best performing StackingRegressor() ended up being that which ensembled the four individual models and the one LinearRegression() model using MARS model of Earth(max_degree = 1) as the metamodel. This yielded a test RMSE and MAE of 0.6397114734243629 and 0.3464615384615385, respectively. These scores were better than each of the models individual performance, but did not outperform the previous best VotingRegressor() model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22a5f3",
   "metadata": {},
   "source": [
    "### Ensemble of ensembled models\n",
    "\n",
    "If you are creating multiple stacking ensembles *(based on different metamodels)*, you may ensemble them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e162c6",
   "metadata": {},
   "source": [
    "We only used MARS with Earth(max_degree = 1) as the metamodel since this was the best performing one in the previous attempts at StackingRegressor() ensembling the individual models and also did not have as high of variance as other potential models to use as metamodels. This yielded a test RMSE = 0.6406727233631369 and test MAE =  0.3464615384615385. As such, this did not perform as well as the simple StackingRegressor() ensembling individual tuned models using a MARS metamodel nor the VotingRegressor() with just three of the individual models, which ultimately performed the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46343d",
   "metadata": {},
   "source": [
    "## Limitations of the model with regard to prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ead90",
   "metadata": {},
   "source": [
    "Are you confident that you found the optimal hyperparameter values for each of your individual models, and that your individual models cannot be better tuned? Or, are there any models that could be better tuned if you had more time / resources, but you are limited by the amount of time you can spend on the course project *(equivalent to one assignment)*? If yes, then which models could be better tuned and how?\n",
    "\n",
    "Will it be possible / convenient / expensive for the stakeholders to collect the data relating to the predictors in the model. Using your model, how soon will the stakeholder be able to predict the outcome before the outcome occurs. For example, if the model predicts the number of bikes people will rent in Evanston on a certain day, then how many days before that day will your model be able to make the prediction. This will depend on how soon the data that your model uses becomes available. If you are predicting election results, how many days / weeks / months / years before the election can you predict the results. \n",
    "\n",
    "When will your model become too obsolete to be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b941f87",
   "metadata": {},
   "source": [
    "For most of the individual models, we are confident that we found the optimal hyperparameter values. Specifically, for MARS, DecisionTreeRegressor(), and RandomForestRegressor() we have a high degree of confidence in this. Likely there might be some improvements possible for XGBRegressor() that were limited by the time we could spend on the course project. Further, there were likely further tuning of hyperparameters that would be computationally expensive but possibly yield improvements for our StackingRegressor() metamodels and some more tuning of those hyperparameters.\n",
    "It will be possible for stakeholders to collect the data relating to the predictors in the model. Often times, wine samples will be obtained from labs for these same physicochemical properties to ensure consistency, with these able to be used as observations for our model with all predictors. Stakeholders can attempt to find a combination of predictor values that yields a higher quality and then attempt to synthesize a Vinho Verde wine with that combination of physicochemical properties. Although this is more difficult, another application could be testing for projected quality of wine to determine which wine to sell at higher prices, what the predicted consumer response to the wine will be, and other factors that quality of wine might impact. \n",
    "Our model will become too obsolete to be useful if the data available is not for Vinho Verde wine with the same physicochemical properties as predictors. Because the model was trained on Vinho Verde wine, any other wine would need to have a separate model trained on the same predictors with a different train dataset. Further, if wine has already been graded by wine experts in a qualitative manner, then this can still be supplemental to those evaluations but with its impact slightly diminished and possibly used more for corroboration of quality scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6026cb7",
   "metadata": {},
   "source": [
    "## Other sections *(optional)*\n",
    "\n",
    "You are welcome to introduce additional sections or subsections, if required, to address any specific aspects of your project in detail. For example, you may briefly discuss potential future work that the research community could focus on to make further progress in the direction of your project's topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a185cb",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations to stakeholder(s)\n",
    "\n",
    "What conclusions do you draw based on your model? You may draw conclusions based on prediction accuracy, or other performance metrics.\n",
    "\n",
    "How do you use those conclusions to come up with meaningful recommendations for stakeholders? The recommendations must be action-items for stakeholders that they can directly implement without any further analysis. Be as precise as possible. The stakeholder(s) are depending on you to come up with practically implementable recommendations, instead of having to think for themselves.\n",
    "\n",
    "If your recommendations are not practically implementable by stakeholders, how will they help them? Is there some additional data / analysis / domain expertise you need to do to make the recommendations implementable? \n",
    "\n",
    "Do the stakeholder(s) need to be aware about some limitations of your model? Is your model only good for one-time use, or is it possible to update your model at a certain frequency (based on recent data) to keep using it in the future? If it can be used in the future, then for how far into the future?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b0ddc",
   "metadata": {},
   "source": [
    "From this, we can conclude that by using physicochemical properties, wine consumers and critics can get a reasonably accurate standard of comparison for the quality of the wine (regardless of whether it is red or white). Our final model had a RMSE of 0.639 and MAE of 0.346, indicating that the average absolute error (difference between predictions and true values) is small enough to not round to another integer. In other words, our prediction accuracy is good enough to use! Additionally, we found that the physicochemical features that are most important in predicting quality are type of wine and alcohol.  \n",
    "\n",
    "In terms of action items, given our model accuracy, Vinho Verde wine producers can input physicochemical property data for different wines into our predictive ensemble model and get wine quality predictions with absolute differences that round to a single quality score. Although the dataset is only applicable to Vinho Verde wines, using new data for other regional wine varieties with the same techniques can yield similar results and accuracy in predictions. Similarly, consumers and tasters of Vinho Verde wine can input  physicochemical property data for different wines into our predictive ensemble model and get an accurate understanding of the quality of wine they are drinking and/or purchasing. In general, we believe that our model is pretty interpretable and our recommendations are very implementable. \n",
    "\n",
    "Our model definitely is capable of being updated and is not only good for one-time use. It’s possible to continue updating the data with more ratings (since it’s currently only the median of three wine expert evaluations). So as more trusted wine expert evaluations are conducted, we can add that to our dataset and redevelop the model to make it more accurate to current industry ratings/trends. Given this approach and the ability for our model to continuously adapt and improve based on new information/ratings, our model can be used pretty far into the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca45613",
   "metadata": {},
   "source": [
    "Add details of each team member's contribution, other than the models contributed, in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505da5c",
   "metadata": {},
   "source": [
    "<html>\n",
    "<style>\n",
    "table, td, th {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "\n",
    "table {\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "th {\n",
    "  text-align: left;\n",
    "}\n",
    "    \n",
    "\n",
    "</style>\n",
    "<body>\n",
    "\n",
    "<h2>Individual contribution</h2>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <colgroup>\n",
    "       <col span=\"1\" style=\"width: 15%;\">\n",
    "       <col span=\"1\" style=\"width: 20%;\">\n",
    "       <col span=\"1\" style=\"width: 25%;\">\n",
    "       <col span=\"1\" style=\"width: 40%;\">\n",
    "    </colgroup>\n",
    "  <tr>\n",
    "    <th>Team member</th>\n",
    "    <th>Individual Model</th>\n",
    "    <th>Work other than individual model</th>    \n",
    "    <th>Details of work other than individual model</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Cindy Wu</td>\n",
    "    <td>MARS</td>\n",
    "    <td>Data cleaning and EDA</td>    \n",
    "    <td>Imputed missing values and visualized data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Michael Kim</td>\n",
    "    <td>Bagged Decision Trees</td>\n",
    "    <td>Ensembling</td>    \n",
    "    <td>Stacking ensembles and voting ensemble</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Sabrina Kozarovitsky</td>\n",
    "    <td>Random forest</td>\n",
    "    <td>Variable selection and voting ensemble</td>    \n",
    "    <td>Variable selection based on feature importance, created initial voting ensemble based on all models</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Keaton Olds</td>\n",
    "    <td>XGBoost</td>\n",
    "    <td>Data Preparation</td>    \n",
    "    <td>Innovative ensemble & stacking ensemble</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1cafe",
   "metadata": {},
   "source": [
    "## References {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb1aad",
   "metadata": {},
   "source": [
    "List and number all bibliographical references. When referenced in the text, enclose the citation number in square brackets, for example [1].\n",
    "\n",
    "[1] jakubzenonkujawa. (2023, February 19). Wine Quality Prediction. Kaggle. Retrieved June 7, 2023, from https://www.kaggle.com/code/jakubzenonkujawa/wine-quality-prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831751c",
   "metadata": {},
   "source": [
    "## Appendix {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d374d",
   "metadata": {},
   "source": [
    "You may put additional stuff here as Appendix. You may refer to the Appendix in the main report to support your arguments. However, the appendix section is unlikely to be checked while grading, unless the grader deems it necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0ccdf",
   "metadata": {},
   "source": [
    "### Distribution of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014a5c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#red wine dataset\n",
    "red.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ca3f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888a17f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.854788</td>\n",
       "      <td>0.278241</td>\n",
       "      <td>0.334192</td>\n",
       "      <td>6.391415</td>\n",
       "      <td>0.045772</td>\n",
       "      <td>35.308085</td>\n",
       "      <td>138.360657</td>\n",
       "      <td>0.994027</td>\n",
       "      <td>3.188267</td>\n",
       "      <td>0.489847</td>\n",
       "      <td>10.514267</td>\n",
       "      <td>5.877909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.100795</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>5.072058</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>17.007137</td>\n",
       "      <td>42.498065</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>0.114126</td>\n",
       "      <td>1.230621</td>\n",
       "      <td>0.885639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.991723</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.993740</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
       "mean        6.854788          0.278241     0.334192        6.391415   \n",
       "std         0.843868          0.100795     0.121020        5.072058   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.320000        5.200000   \n",
       "75%         7.300000          0.320000     0.390000        9.900000   \n",
       "max        14.200000          1.100000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
       "mean      0.045772            35.308085            138.360657     0.994027   \n",
       "std       0.021848            17.007137             42.498065     0.002991   \n",
       "min       0.009000             2.000000              9.000000     0.987110   \n",
       "25%       0.036000            23.000000            108.000000     0.991723   \n",
       "50%       0.043000            34.000000            134.000000     0.993740   \n",
       "75%       0.050000            46.000000            167.000000     0.996100   \n",
       "max       0.346000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
       "mean      3.188267     0.489847    10.514267     5.877909  \n",
       "std       0.151001     0.114126     1.230621     0.885639  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.090000     0.410000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.550000    11.400000     6.000000  \n",
       "max       3.820000     1.080000    14.200000     9.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#white wine dataset\n",
    "white.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72939f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7016f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
